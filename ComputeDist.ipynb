{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pivottablejs\n",
    "import statsmodels\n",
    "import json\n",
    "import math\n",
    "import imageio\n",
    "import itertools\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'magma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mTurk = True\n",
    "\n",
    "if mTurk:\n",
    "    all_keys = ['c83-101_0A', 'c83-101_0B', 'c83-101_14A', 'c83-101_14B', 'c83-101_29A', 'c83-101_29B', \n",
    "                'c83-101_43A', 'c83-101_43B', 'c83-101_57A', 'c83-101_57B', 'c83-101_71A', 'c83-101_71B', \n",
    "                'c83-101_86A', 'c83-101_86B', 'c83-101_100A', 'c83-101_100B', 'c11-5_0A', 'c11-5_0B', \n",
    "                'c11-5_14A', 'c11-5_14B', 'c11-5_29A', 'c11-5_29B', 'c11-5_43A', 'c11-5_43B', \n",
    "                'c11-5_57A', 'c11-5_57B', 'c11-5_71A', 'c11-5_71B', 'c11-5_86A', 'c11-5_86B', \n",
    "                'c11-5_100A', 'c11-5_100B', 'c102-109_0A', 'c102-109_0B', 'c102-109_14A', 'c102-109_14B', \n",
    "                'c102-109_29A', 'c102-109_29B', 'c102-109_43A', 'c102-109_43B', 'c102-109_57A', 'c102-109_57B', \n",
    "                'c102-109_71A', 'c102-109_71B', 'c102-109_86A', 'c102-109_86B', 'c102-109_100A', 'c102-109_100B', \n",
    "                'c17-85_0A', 'c17-85_0B', 'c17-85_14A', 'c17-85_14B', 'c17-85_29A', 'c17-85_29B', \n",
    "                'c17-85_43A', 'c17-85_43B', 'c17-85_57A', 'c17-85_57B', 'c17-85_71A', 'c17-85_71B', \n",
    "                'c17-85_86A', 'c17-85_86B', 'c17-85_100A', 'c17-85_100B', 'c47-34_0A', 'c47-34_0B', \n",
    "                'c47-34_14A', 'c47-34_14B', 'c47-34_29A', 'c47-34_29B', 'c47-34_43A', 'c47-34_43B', \n",
    "                'c47-34_57A', 'c47-34_57B', 'c47-34_71A', 'c47-34_71B', 'c47-34_86A', 'c47-34_86B', \n",
    "                'c47-34_100A', 'c47-34_100B', 'c40-20_0A', 'c40-20_0B', 'c40-20_14A', 'c40-20_14B', \n",
    "                'c40-20_29A', 'c40-20_29B', 'c40-20_43A', 'c40-20_43B', 'c40-20_57A', 'c40-20_57B', \n",
    "                'c40-20_71A', 'c40-20_71B', 'c40-20_86A', 'c40-20_86B', 'c40-20_100A', 'c40-20_100B', \n",
    "                'c79-97_0A', 'c79-97_0B', 'c79-97_14A', 'c79-97_14B', 'c79-97_29A', 'c79-97_29B', \n",
    "                'c79-97_43A', 'c79-97_43B', 'c79-97_57A', 'c79-97_57B', 'c79-97_71A', 'c79-97_71B', \n",
    "                'c79-97_86A', 'c79-97_86B', 'c79-97_100A', 'c79-97_100B', 'c56-68_0A', 'c56-68_0B', \n",
    "                'c56-68_14A', 'c56-68_14B', 'c56-68_29A', 'c56-68_29B', 'c56-68_43A', 'c56-68_43B', \n",
    "                'c56-68_57A', 'c56-68_57B', 'c56-68_71A', 'c56-68_71B', 'c56-68_86A', 'c56-68_86B', \n",
    "                'c56-68_100A', 'c56-68_100B']\n",
    "else:\n",
    "    all_keys = ['l2-63_0A', 'l2-63_0B', 'l2-63_14A', 'l2-63_14B', 'l2-63_29A', 'l2-63_29B', 'l2-63_43A', \n",
    "                'l2-63_43B', 'l2-63_57A', 'l2-63_57B', 'l2-63_71A', 'l2-63_71B', 'l2-63_86A', 'l2-63_86B', \n",
    "                'l2-63_100A', 'l2-63_100B', 'l62-58_0A', 'l62-58_0B', 'l62-58_14A', 'l62-58_14B', \n",
    "                'l62-58_29A', 'l62-58_29B', 'l62-58_43A', 'l62-58_43B', 'l62-58_57A', 'l62-58_57B', \n",
    "                'l62-58_71A', 'l62-58_71B', 'l62-58_86A', 'l62-58_86B', 'l62-58_100A', 'l62-58_100B', \n",
    "                'l3-61_0A', 'l3-61_0B', 'l3-61_14A', 'l3-61_14B', 'l3-61_29A', 'l3-61_29B', 'l3-61_43A', \n",
    "                'l3-61_43B', 'l3-61_57A', 'l3-61_57B', 'l3-61_71A', 'l3-61_71B', 'l3-61_86A', 'l3-61_86B', \n",
    "                'l3-61_100A', 'l3-61_100B', 'l53-57_0A', 'l53-57_0B', 'l53-57_14A', 'l53-57_14B', \n",
    "                'l53-57_29A', 'l53-57_29B', 'l53-57_43A', 'l53-57_43B', 'l53-57_57A', 'l53-57_57B', \n",
    "                'l53-57_71A', 'l53-57_71B', 'l53-57_86A', 'l53-57_86B', 'l53-57_100A', 'l53-57_100B', \n",
    "                'l30-24_0A', 'l30-24_0B', 'l30-24_14A', 'l30-24_14B', 'l30-24_29A', 'l30-24_29B', \n",
    "                'l30-24_43A', 'l30-24_43B', 'l30-24_57A', 'l30-24_57B', 'l30-24_71A', 'l30-24_71B', \n",
    "                'l30-24_86A', 'l30-24_86B', 'l30-24_100A', 'l30-24_100B', 'l12-14_0A', 'l12-14_0B', \n",
    "                'l12-14_14A', 'l12-14_14B', 'l12-14_29A', 'l12-14_29B', 'l12-14_43A', 'l12-14_43B', \n",
    "                'l12-14_57A', 'l12-14_57B', 'l12-14_71A', 'l12-14_71B', 'l12-14_86A', 'l12-14_86B', \n",
    "                'l12-14_100A', 'l12-14_100B', 'l59-23_0A', 'l59-23_0B', 'l59-23_14A', 'l59-23_14B', \n",
    "                'l59-23_29A', 'l59-23_29B', 'l59-23_43A', 'l59-23_43B', 'l59-23_57A', 'l59-23_57B', \n",
    "                'l59-23_71A', 'l59-23_71B', 'l59-23_86A', 'l59-23_86B', 'l59-23_100A', 'l59-23_100B', \n",
    "                'l32-9_0A', 'l32-9_0B', 'l32-9_14A', 'l32-9_14B', 'l32-9_29A', 'l32-9_29B', 'l32-9_43A', \n",
    "                'l32-9_43B', 'l32-9_57A', 'l32-9_57B', 'l32-9_71A', 'l32-9_71B', 'l32-9_86A', 'l32-9_86B', \n",
    "                'l32-9_100A', 'l32-9_100B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given an item and a given trials dictionary, retrieve the x and y coordinates\n",
    "\n",
    "def retrieve_coords(coord_dict,item):\n",
    "    coords = str(coord_dict[item])\n",
    "    coords = coords.replace(\"'\", \"\")\n",
    "    coords = coords.replace(\", \", \",\")\n",
    "    x = int(coords.split(',')[0])\n",
    "    y = int(coords.split(',')[1])\n",
    "    return x, y\n",
    "\n",
    "# Given a dictionary, return the keys (all items present in the dict)\n",
    "\n",
    "def retrieve_keys(coord_dict):\n",
    "    out_keys = list(coord_dict.keys())\n",
    "    return sorted(out_keys)\n",
    "\n",
    "# Given a dictionary and two critical items, compute the euclidian distance\n",
    "\n",
    "def compute_dist(coord_dict, item1, item2):\n",
    "    x1, y1 = retrieve_coords(coord_dict, item1)\n",
    "    x2, y2 = retrieve_coords(coord_dict, item2)\n",
    "    dist = np.sqrt(np.square(x1-x2)+np.square(y1-y2))\n",
    "    return dist\n",
    "\n",
    "# Create an n by n pandas dataframe, where n is the total number of keys. If full=None, its blank\n",
    "# if full=df, where df is a given dataframe, it will populate it with the values.\n",
    "# This is useful for switching to np.array and back\n",
    "\n",
    "def create_mat(full_keys, full=None):\n",
    "    dist_mat = pd.DataFrame(data=full, index=full_keys, columns=full_keys)\n",
    "    return dist_mat\n",
    "\n",
    "# Given a pandas dataframe, two items, and a distance between them, this function places the distance appropriately\n",
    "\n",
    "def place_dist(fillable_mat, item1, item2, dist):\n",
    "    fillable_mat.loc[item1,item2] = dist\n",
    "    fillable_mat.loc[item2,item1] = dist\n",
    "    #return fillable_mat\n",
    "\n",
    "# Pull a particular distance given a matrix and a pair of items    \n",
    "\n",
    "def pull_dist(fillable_mat, item1, item2):\n",
    "    out_dist = fillable_mat.loc[item1,item2]\n",
    "    return out_dist\n",
    "\n",
    "# Once pd dataframes have been turned into arrays, this compiles them and computes the mean across 3rd dimension\n",
    "# This is because we will have multiple trials, where some have distances for a particular pair and other's don't\n",
    "# If standard, it will standardize the subjects' distances wrt their own judgments\n",
    "\n",
    "\n",
    "def std_array(in_array):\n",
    "    array = np.array(in_array, np.float64)\n",
    "    out_array = (array - np.nanmean(array)) / np.nanstd(array)\n",
    "    return out_array\n",
    "    \n",
    "\n",
    "\n",
    "def nan_mean(arrays, standard=False):\n",
    "    all_arrs = np.dstack(arrays)\n",
    "    if standard:\n",
    "        all_arrs = (all_arrs - np.nanmean(all_arrs)) / np.nanstd(all_arrs)\n",
    "    avg_arr = np.nanmean(all_arrs, axis=2)\n",
    "    return avg_arr    \n",
    "\n",
    "def nan_median(arrays, standard=False):\n",
    "    all_arrs = np.dstack(arrays)\n",
    "    if standard:\n",
    "        all_arrs = (all_arrs - np.nanmean(all_arrs)) / np.nanstd(all_arrs)\n",
    "    med_arr = np.nanmedian(all_arrs, axis=2)\n",
    "    return med_arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "# read in the data file, then loop through each line (which should be a single subject)\n",
    "# take the first item from each line as the subject, then the third item on as the trialwise data\n",
    "# Find the index where each trial number starts (crit_ind)\n",
    "# Use this to loop through and parse the rt and dictionary\n",
    "# A bunch of ugly code to make the dictionary json readable\n",
    "# Maintain equal dim lists of subject, trial number, rt, dictionary\n",
    "\n",
    "\n",
    "datafiles = [open('MTurkBatch1/testData.txt'), \n",
    "             open('MTurkBatch2/testData.txt'), \n",
    "             open('MTurkBatch3/testData.txt')]\n",
    "# datafile = open('myData3.txt')\n",
    "sub_array = []\n",
    "qual_array = []\n",
    "trial_nums = []\n",
    "times = []\n",
    "dicts = []\n",
    "for datafile in datafiles:\n",
    "    for line in datafile:\n",
    "        qual_code = line.split(',')[0]\n",
    "        turk_code = line.split(',')[1]\n",
    "        trials = np.array(line.split(',')[3:])\n",
    "        # qual_code = 'NA'\n",
    "        # turk_code = line.split(',')[0]\n",
    "        # trials = np.array(line.split(',')[2:])\n",
    "        ntrials = line.count('{')\n",
    "        crit_inds=[]\n",
    "        for i in range(1,ntrials+1):\n",
    "            crit = np.in1d(trials, str(i))\n",
    "            crit_ind = [ind for ind in range(len(trials)) if crit[ind]==True][0]\n",
    "            crit_inds.append(crit_ind)\n",
    "        for i in range(len(crit_inds)):\n",
    "            ind = crit_inds[i]\n",
    "            trial_num = i\n",
    "            time = trials[ind+1]\n",
    "            if i < len(crit_inds)-1:\n",
    "                next_ind = crit_inds[i+1]\n",
    "                loc_dict = str(trials[ind+2:next_ind]).split('{')[-1]\n",
    "            else:\n",
    "                loc_dict = str(trials[ind+2:]).split('{')[-1]\n",
    "            loc_dict = str(loc_dict).split('}')[0]\n",
    "            loc_dict = '{' + loc_dict + '}'\n",
    "            loc_dict = loc_dict.replace(\"\\n\", \"\")\n",
    "            loc_dict = loc_dict.replace(\"' '\", \"', '\")\n",
    "            loc_dict = loc_dict.replace(\"'\", \"\")\n",
    "            loc_dict = json.loads(loc_dict)\n",
    "            sub_array.append(turk_code)\n",
    "            qual_array.append(qual_code)\n",
    "            trial_nums.append(trial_num)\n",
    "            times.append(time)\n",
    "            dicts.append(loc_dict)\n",
    "sub_array = np.array(sub_array)\n",
    "qual_array = np.array(qual_array)\n",
    "trial_nums = np.array(trial_nums)\n",
    "times = np.array(times)\n",
    "dicts = np.array(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the lists derived above into a pandas array that's more human readable\n",
    "subset_trials = False\n",
    "data_full = pd.DataFrame(np.transpose(np.vstack((qual_array, sub_array, trial_nums, times, dicts))), \n",
    "                         columns=['qual','sub','trial','time','coords'])\n",
    "rts = np.array(data_full.loc[:,'time'], dtype=np.int64)\n",
    "rts = rts[:]\n",
    "rts = rts/1000/60\n",
    "print(np.mean(rts))\n",
    "long_rts = [i for i in range(len(rts)) if rts[i] > 3]\n",
    "if subset_trials:\n",
    "    data_full = data_full.iloc[long_rts]\n",
    "print(data_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To produce all the keys\n",
    "\n",
    "nums = [0, 14, 29, 43, 57, 71, 86, 100]\n",
    "\n",
    "# low level\n",
    "# filter_pairs = [(2, 63), (62, 58),  (3, 61), (53, 57), (30, 24), (12, 14), (59, 23), (32, 9)]\n",
    "filter_pairs = [(83, 101), (11, 5), (102, 109), (17, 85), (47, 34), (40, 20), (79, 97), (56, 68)]\n",
    "\n",
    "fnames = []\n",
    "for chan_pair in range(len(nums)):\n",
    "    chan1, chan2 = filter_pairs[chan_pair]\n",
    "    for pairing in range(len(nums)):\n",
    "        labeler = nums[pairing]\n",
    "        # A = str('Pilot_Pairs/l' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'A.png')\n",
    "        # B = str('Pilot_Pairs/l' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'B.png')\n",
    "        A = str('Pilot_Pairs/c' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'A.png')\n",
    "        B = str('Pilot_Pairs/c' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'B.png')\n",
    "        fnames.append(A)\n",
    "        fnames.append(B)\n",
    "    \n",
    "# fnames is the whole list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = []\n",
    "subjects = np.unique(data_full['sub'])\n",
    "for subject in subjects:\n",
    "    arrays = []\n",
    "    sub_data = data_full[data_full['sub']==subject]\n",
    "    if subset_trials:\n",
    "        sub_trials = np.unique(sub_data['trial'])\n",
    "    else:\n",
    "        sub_trials = np.arange(sub_data.shape[0])\n",
    "    print(\"subject {}, completed {} trials\".format(subject,sub_trials.shape[0]))\n",
    "    for trial in sub_trials:\n",
    "        new_arr = create_mat(all_keys, full=None)\n",
    "        coord_dict = sub_data.loc[sub_data['trial'] == trial, 'coords'].iloc[0]\n",
    "        key = retrieve_keys(coord_dict)\n",
    "        for i in key:\n",
    "            if i in all_keys:\n",
    "                for j in key:\n",
    "                    if j in all_keys:\n",
    "                        dist_measure = compute_dist(coord_dict, i, j)\n",
    "                        place_dist(new_arr, i, j, dist_measure)\n",
    "        new_arr = std_array(new_arr)\n",
    "        arrays.append(np.array(new_arr))\n",
    "    arrays = np.array(arrays, np.float64)\n",
    "    countingthinger = np.count_nonzero(~np.isnan(arrays), axis=0)\n",
    "    countingthinger2 = create_mat(all_keys, full=countingthinger)\n",
    "    do_mean = nan_mean(arrays, standard=False)\n",
    "    sub_avg = create_mat(all_keys, full=do_mean)\n",
    "    all_data.append(np.array(do_mean))\n",
    "all_data = np.array(all_data, np.float64)\n",
    "all_count = np.count_nonzero(~np.isnan(all_data), axis=0)\n",
    "all_mean = nan_mean(all_data, standard=False)\n",
    "all_counts = create_mat(all_keys, full=all_count)\n",
    "all_subs = create_mat(all_keys, full=all_mean)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full negative distance matrix of all images, we only really care about the big blocks on the diagonal where we specifically set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure()\n",
    "im = plt.matshow(all_mean*-1)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the images that we actually set the values for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0, 14, 29, 43, 57, 71, 86, 100]\n",
    "filter_pairs = [(2, 63), (62, 58),  (3, 61), (53, 57), (30, 24), (12, 14), (59, 23), (32, 9)]\n",
    "filter_pairs = [(83, 101), (11, 5), (102, 109), (17, 85), (47, 34), (40, 20), (79, 97), (56, 68)]\n",
    "\n",
    "all_outs = []\n",
    "for chan_pair in range(len(nums)):\n",
    "    chan1, chan2 = filter_pairs[chan_pair]\n",
    "    outs = []\n",
    "    for pairing in range(len(nums)):\n",
    "        labeler = nums[pairing]\n",
    "        # A = str('l' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'A')\n",
    "        # B = str('l' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'B')\n",
    "        A = str('c' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'A')\n",
    "        B = str('c' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'B')\n",
    "        output = pull_dist(all_subs, A, B)\n",
    "        outs.append(output)\n",
    "    outs = outs[::-1]\n",
    "    all_outs.append(outs)\n",
    "all_outs = np.array(all_outs)\n",
    "all_outs = all_outs*-1\n",
    "\n",
    "plt.clf()\n",
    "im = plt.matshow(all_outs)\n",
    "plt.colorbar(im)\n",
    "plt.show()\n",
    "\n",
    "print('ACROSS THE TOP: 0 is the most dissimilar (by the model), and 7 is the most similar')\n",
    "print('DOWN THE SIDE: The numbers are meaningless, its simply the different sets of endpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spear = False\n",
    "\n",
    "import pickle\n",
    "# layer = 'conv2d1'\n",
    "layer = 'mixed4e'\n",
    "def open_dict(fname):\n",
    "    with open(fname + '.pkl', 'rb') as reader:\n",
    "        return pickle.load(reader)\n",
    "    \n",
    "#corrs = open_dict('corrs')\n",
    "#corrs = open_dict('low'+str(layer))\n",
    "corrs = open_dict(layer)\n",
    "plt.clf()\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (12,4))\n",
    "im1 = ax1.matshow(-all_mean)\n",
    "ax1.get_xaxis().set_ticks([])\n",
    "ax1.get_yaxis().set_ticks([])\n",
    "ax1.set_title('Distance Ratings')\n",
    "\n",
    "im2 = ax2.matshow(corrs)\n",
    "ax2.get_xaxis().set_ticks([])\n",
    "ax2.get_yaxis().set_ticks([])\n",
    "ax2.set_title('Inception Output')\n",
    "\n",
    "im3 = ax3.matshow(all_counts)\n",
    "ax3.get_xaxis().set_ticks([])\n",
    "ax3.get_yaxis().set_ticks([])\n",
    "ax3.set_title('Number of Ratings')\n",
    "plt.show()\n",
    "\n",
    "corrs.shape\n",
    "all_mean.shape\n",
    "\n",
    "flat_corrs = corrs.flatten()\n",
    "flat_means = all_mean.flatten()\n",
    "\n",
    "all_corr_pile = pd.DataFrame(np.transpose(np.vstack((flat_corrs, flat_means))))\n",
    "all_corr = all_corr_pile.corr()\n",
    "if spear:\n",
    "    full_matt_corr = np.round(scipy.stats.spearmanr(all_corr_pile)[0],3)\n",
    "else:\n",
    "    full_mat_corr = -np.round(np.array(all_corr)[0,1],2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('CORRELATION BETWEEN FULL INCEPTION STRUCTURE AND DISTANCE RATING STRUCTURE: {}'.format(full_mat_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_spreads = []\n",
    "spreads = np.empty((0, 8), float)\n",
    "for i in range(0,128,16):\n",
    "    spread = []\n",
    "    subarray = corrs[i:i+16,i:i+16]\n",
    "    for xy in range(0,16,2):\n",
    "        spread.append(subarray[(xy),(xy+1)])\n",
    "    spread = spread[::-1]\n",
    "    spread = np.expand_dims(np.array(spread),axis=0)\n",
    "    spreads = np.concatenate((spreads, spread))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aims = np.array([0,0.14,0.28,0.43,0.57,0.71,0.86,1])\n",
    "aims = aims[::-1]\n",
    "#aims = np.array([0.25]*8)\n",
    "aims = np.vstack((aims, aims, aims, aims, aims, aims, aims, aims))\n",
    "aims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EVALUATING BEHAVIORAL SORTING IN PILOT SUBJECTS, n = {}'.format(len(subjects)))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4))\n",
    "im1 = ax1.matshow(all_outs)\n",
    "im2 = ax2.matshow(spreads, vmin = 0, vmax = 1)\n",
    "im3 = ax3.matshow(aims, vmin = 0, vmax = 1)\n",
    "ax1.set_xlabel('Increasing Similarity Aim-->')\n",
    "ax1.set_ylabel('Different Channel Pairs')\n",
    "ax1.get_xaxis().set_ticks([])\n",
    "ax1.get_yaxis().set_ticks([])\n",
    "ax1.set_title('Normalized Euclidian Distance (x -1)')\n",
    "ax2.set_xlabel('Increasing Similarity Aim-->')\n",
    "ax2.set_ylabel('Different Channel Pairs')\n",
    "ax2.get_xaxis().set_ticks([])\n",
    "ax2.get_yaxis().set_ticks([])\n",
    "ax2.set_title('Actual Inception Correlations')\n",
    "ax3.set_xlabel('Increasing Similarity Aim-->')\n",
    "ax3.set_ylabel('Different Channel Pairs')\n",
    "ax3.get_xaxis().set_ticks([])\n",
    "ax3.get_yaxis().set_ticks([])\n",
    "ax3.set_title('Inception Aims')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "coll_outs = np.mean(all_outs, axis=0)\n",
    "coll_spreads = np.mean(spreads, axis=0)\n",
    "\n",
    "\n",
    "flat_outs = all_outs.flatten()\n",
    "flat_spreads = spreads.flatten()\n",
    "flat_aims = aims.flatten()\n",
    "\n",
    "cross_domain = np.round(np.corrcoef(flat_outs,flat_spreads)[0,1],3)\n",
    "if spear:\n",
    "    cross_domain = np.round(scipy.stats.spearmanr(flat_outs,flat_spreads)[0],3)\n",
    "print('CORRELATION BETWEEN BEHAVIORAL SORTING AND ACTUAL INCEPTION STRUCTURE: {}'.format(cross_domain))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('COMPARING ACROSS MATRICES')\n",
    "print('----------------------------------------------')\n",
    "print('From left to right:')\n",
    "print('Do distance measures for all pairs track the model structure?')\n",
    "print('The same, but collapsed across possible axes (different images, same parameter aims)')\n",
    "print('Does Inception produce what we want it to?')\n",
    "print('Do distance measures for all pairs track our intitial aims?')\n",
    "print('----------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize = (12,4))\n",
    "ax1.scatter(flat_spreads,flat_outs, c=(0,1,0.5))\n",
    "ax1.set_title('All 8 Channel/Channel Axes')\n",
    "ax1.set_xlabel('Actual Inception Correlations')\n",
    "ax1.set_ylabel('Normalized Euclidian Distance (x -1)')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([-1, 1.75])\n",
    "ax2.scatter(coll_spreads,coll_outs, c=(1,0.75,0))\n",
    "ax2.set_title('Averaged Across Axes')\n",
    "ax2.set_xlabel('Actual Inception Correlations')\n",
    "ax2.set_ylabel('Normalized Inverse Distance (x -1)')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([-1, 1.75])\n",
    "ax3.scatter(flat_aims,flat_spreads, c=(0,0.5,1))\n",
    "ax3.set_title('Stim Creation (Aim versus Actual)')\n",
    "ax3.set_xlabel('Intended Inception Correlations')\n",
    "ax3.set_ylabel('Actual Inception Correlations')\n",
    "ax3.set_xlim([0, 1])\n",
    "ax3.set_ylim([0, 1])\n",
    "ax4.scatter(flat_aims,flat_outs, c=(1,0,0.5))\n",
    "ax4.set_title('Aim versus Distance')\n",
    "ax4.set_xlabel('Intended Inception Correlations')\n",
    "ax4.set_ylabel('Normalized Euclidian Distance (x -1)')\n",
    "ax4.set_xlim([0, 1])\n",
    "ax4.set_ylim([-1, 1.75])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "aim_sort = np.round(np.corrcoef(flat_outs,flat_aims)[0,1],3)\n",
    "qual_sort = np.round(np.corrcoef(flat_aims,flat_spreads)[0,1],3)\n",
    "if spear:\n",
    "    aim_sort = np.round(scipy.stats.spearmanr(flat_outs,flat_aims)[0],3)\n",
    "    qual_sort = np.round(scipy.stats.spearmanr(flat_aims,flat_spreads)[0],3)\n",
    "print('CORRELATION BETWEEN INTENDED INCEPTION STRUCTURE AND BEHAVIORAL SORTING: {}'.format(aim_sort))\n",
    "print('CORRELATION BETWEEN INTENDED INCEPTION STRUCTURE AND ACTUAL INCEPTION STRUCTURE: {}'.format(qual_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [0, 14, 29, 43, 57, 71, 86, 100]\n",
    "#filter_pairs = [(2, 63), (62, 58),  (3, 61), (53, 57), (30, 24), (12, 14), (59, 23), (32, 9)]\n",
    "\n",
    "filter_pairs = [(83, 101), (11, 5), (102, 109), (17, 85), (47, 34), (40, 20), (79, 97), (56, 68)]\n",
    "\n",
    "\n",
    "derp_keys = ['0', '14', '29', '43', '57', '71', '86', '100']\n",
    "\n",
    "substack = []\n",
    "for i in range(all_data.shape[0]):\n",
    "    trim = all_data[i,:,:]\n",
    "    trim = create_mat(all_keys, full=trim)\n",
    "    each_sub = []\n",
    "    for chan_pair in range(len(nums)):\n",
    "        chan1, chan2 = filter_pairs[chan_pair]\n",
    "        outs = []\n",
    "        for pairing in range(len(nums)):\n",
    "            labeler = nums[pairing]\n",
    "            # A = str('l' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'A')\n",
    "            # B = str('l' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'B')\n",
    "            A = str('c' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'A')\n",
    "            B = str('c' + str(chan1)+'-'+str(chan2)+'_'+str(labeler)+'B')\n",
    "            output = pull_dist(trim, A, B)\n",
    "            outs.append(output)\n",
    "        each_sub.append(outs)\n",
    "    each_sub = np.array(each_sub)\n",
    "    outtt = create_mat(derp_keys, full=each_sub)\n",
    "    outtt.to_csv('subject_'+str(i)+'.csv')\n",
    "    flat_sub = each_sub.flatten()\n",
    "    substack.append(flat_sub)\n",
    "substack = np.array(substack)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derp = spreads.flatten()\n",
    "derp = np.expand_dims(derp,0)\n",
    "derps = np.tile(derp,(len(subjects),1))\n",
    "colors = [[0.294,0.608,0.478,0.1],[0.792,0.404,0.153,0.1],\n",
    "          [0.455,0.439,0.682,0.1],[0.831,0.239,0.533,0.1],\n",
    "          [0.459,0.643,0.227,0.1],[0.867,0.678,0.231,0.1],\n",
    "          [0.62,0.471,0.192,0.1],[0.4,0.4,0.4,0.1]]*2\n",
    "\n",
    "add = [0,0,0,0.4]\n",
    "\n",
    "all_ratings = substack.flatten()\n",
    "all_incepts = derps.flatten()\n",
    "all_all = pd.DataFrame(np.transpose(np.vstack((all_ratings, all_incepts))))\n",
    "all_all = all_all.corr()\n",
    "\n",
    "substack2 = pd.DataFrame(np.transpose(substack))\n",
    "substack3 = substack2.corr()\n",
    "if spear:\n",
    "    substack3 = np.array(scipy.stats.spearmanr(substack2))[0]\n",
    "summ = 0\n",
    "lenn = 0\n",
    "for i in range(1,9):\n",
    "    interim = np.diag(np.array(substack3), k=i)\n",
    "    summ += np.sum(interim)\n",
    "    lenn += interim.shape[0]\n",
    "inter_rater = summ / lenn\n",
    "\n",
    "\n",
    "\n",
    "idx = np.isfinite(all_incepts) & np.isfinite(all_ratings)\n",
    "trendline = np.polyfit(all_incepts[idx], all_ratings[idx], 1)\n",
    "\n",
    "\n",
    "trend_x = np.linspace(0.0, 1.0, num=20)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax1.scatter(all_incepts,all_ratings, s = 200, c=(0,1,0.75,0.1), edgecolors=(0,0.64,0.32,0.5))\n",
    "ax1.plot(trend_x, trendline[0] * trend_x + trendline[1], c = (0,0.64,0.32,0.5), linewidth=10)\n",
    "ax1.set_title('Every Subject, Every Image', fontsize=20)\n",
    "ax1.set_xlabel('Ground Truth Inception Correlation', fontsize=20)\n",
    "ax1.set_ylabel('Normalized Inverse Distance', fontsize=20)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([-2.5, 2.5])\n",
    "for i in range(len(subjects)):\n",
    "    col = np.append(np.random.random(3),0.1)\n",
    "    ax2.scatter(derps[0],substack[i], s = 200, c=col, edgecolors=np.add(col, add))\n",
    "    idx = np.isfinite(derps[0]) & np.isfinite(substack[i])\n",
    "    trendline = np.polyfit(derps[0][idx], substack[i][idx], 1)\n",
    "    ax2.plot(trend_x, trendline[0] * trend_x + trendline[1], c = np.add(col, add), linewidth=7)\n",
    "ax2.set_title('Individual Subject Trendlines', fontsize=20)\n",
    "ax2.set_xlabel('Ground Truth Inception Correlation', fontsize=20)\n",
    "ax2.set_ylabel('Normalized Inverse Distance', fontsize=20)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([-2.5, 2.5])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('ALL SUBJECTS AND POINTS CORRELATION: {}, AVERAGE INTER-RATER CORRELATION: {}'.format(np.round(np.array(all_all)[0,1],2), np.round(inter_rater,2)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(-substack)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> f5b81b04f83d1bf11d9d1463df16374dd63223d3
   "metadata": {},
   "outputs": [],
   "source": [
    "thorough = False\n",
    "\n",
    "iters=100000\n",
    "indices=np.arange(0,8)\n",
    "perms = list(itertools.permutations(indices))\n",
    "best_corr = 0\n",
    "best_lineup = []\n",
    "best_indices = []\n",
    "all_corrs = []\n",
    "all_lines = []\n",
    "all_indices = []\n",
    "all_std = []\n",
    "for perm in perms:\n",
    "    picked = []\n",
    "    for j in range(8):\n",
    "        picked.append(all_outs[perm[j],j])\n",
    "    s,i,r,p,std = linregress(range(8), picked)\n",
    "    if thorough:\n",
    "        all_corrs.append(np.around(r,6))\n",
    "        all_lines.append(np.around(picked,2))\n",
    "        all_indices.append(perm)\n",
    "        all_std.append(np.around(std,6))\n",
    "    if r < best_corr:\n",
    "        best_corr = r\n",
    "        best_lineup = picked\n",
    "        best_indices = perm\n",
    "if thorough:\n",
    "    altogether = pd.DataFrame([all_indices, all_corrs, all_lines, all_std]).transpose()\n",
    "    altogether.columns = ['indices', 'corr', 'vals', 'stdev']\n",
    "    top_corr = altogether.sort_values(by=['stdev']).iloc[:50]\n",
    "    tightest = top_corr.sort_values(by=['corr']).iloc[:25]\n",
    "    print(tightest)\n",
    "print(\"indices: {}, slope: {}, BEST LINEUP = {}\".format(best_indices, np.around(best_corr,2), np.around(best_lineup,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cherry = np.array([all_outs[best_indices[0],0], all_outs[best_indices[1],1], \n",
    "                   all_outs[best_indices[2],2], all_outs[best_indices[3],3], \n",
    "                   all_outs[best_indices[4],4], all_outs[best_indices[5],5], \n",
    "                   all_outs[best_indices[6],6], all_outs[best_indices[7],7]])\n",
    "channnns = [(83, 101), (11, 5), (102, 109), (17, 85), (47, 34), (40, 20), (79, 97), (56, 68)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry = np.expand_dims(cherry,0)\n",
    "plt.matshow(cherry)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(cherry[0])\n",
    "plt.show()\n",
    "cherry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(all_outs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> f5b81b04f83d1bf11d9d1463df16374dd63223d3
   "outputs": [],
   "source": [
    "def reproduce_trial(subject, trial, save=False, show=False):\n",
    "    master = Image.new(\"RGB\", (1000,1000), (0,0,0))\n",
    "    draw = ImageDraw.Draw(master)\n",
    "    draw.ellipse((96, 96, 904, 904), fill=(255,255,255))\n",
    "    draw.ellipse((104, 104, 896, 896), fill=(0,0,0))\n",
    "    sub_data = data_full[data_full['sub']==subject]\n",
    "    coord_dict = sub_data.loc[sub_data['trial'] == trial, 'coords'].iloc[0] \n",
    "    rt = sub_data.loc[sub_data['trial'] == trial, 'time'].iloc[0] \n",
    "    keys = retrieve_keys(coord_dict)\n",
    "    for key in keys:\n",
    "        x, y = retrieve_coords(coord_dict,key)\n",
    "        x -= 100\n",
    "        y -= 100\n",
    "        currIm = Image.open(str(key) + '.png')\n",
    "        currIm.thumbnail((120,120))\n",
    "        master.paste(currIm, (x,y), currIm)\n",
    "    arrangement = np.asarray(master)\n",
    "    if show:\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(arrangement)\n",
    "        plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(str(subject)+'_'+str(trial)+'.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return rt, arrangement\n",
    "    \n",
    "def giffify_trials(subject, time_per_trial='match', total_time=20, verbose=False):\n",
    "    sub_data = data_full[data_full['sub']==subject]\n",
    "    sub_trials = sub_data.shape[0]\n",
    "    images=[]\n",
    "    rts=[]\n",
    "    for trial in range(sub_trials):\n",
    "        clear_output(wait=True) if verbose else print('',end='')\n",
    "        print('subject: {}, trial {}/{} done'.format(subject, trial+1, sub_trials)) if verbose else print('',end='')\n",
    "        rt, arrangement = reproduce_trial(subject, trial)\n",
    "        images.append(arrangement)\n",
    "        rts.append(int(rt))\n",
    "    if time_per_trial == 'match':\n",
    "        whole_time = np.sum(rts)\n",
    "        times = list((np.array(rts) / whole_time) * 20)\n",
    "    else:\n",
    "        times = time_per_trial\n",
    "    imageio.mimsave(str(subject)+'_all.gif', images, duration=times)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt, arrangement = reproduce_trial('5kwO4weo', 5, save=False, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giffify_trials('Qzahogax', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
